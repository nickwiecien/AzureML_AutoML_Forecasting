{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10afd4a3",
   "metadata": {},
   "source": [
    "# 02. Azure ML Pipeline Creation - AutoML for Time-Series Forecasting\n",
    "This notebook demonstrates creation of an Azure ML pipeline designed to load data from an AML-linked Datastore, split into train/forecasting datasets, submit an AutoML job, and then register the model into the workspace and save a forward-looking forecast. <i>Run this notebook after running </i>`01_Setup_AML_Env.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77e17a",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e72e32",
   "metadata": {},
   "source": [
    "### Connect to AML workspace and get reference to training compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "cluster_name = 'cpucluster'\n",
    "compute_target = ComputeTarget(workspace=ws, name=cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e548b2",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "\n",
    "Here, we are using an existing conda environment yaml file (`./automl_env.yml`) to create our environment and also register it to the AML workspace so that it can be used for future forecasting operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59720aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment.from_conda_specification(file_path='automl_env.yml', name='AutoMLEnv')\n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a4528",
   "metadata": {},
   "source": [
    "### Get reference to default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde100f",
   "metadata": {},
   "source": [
    "### Define Input and Output Datasets\n",
    "Retrieve references to two registered datasets to be consumed as inputs by the pipeline, and configure an `OutputFileDatasetConfig` to point to a location in blob storage where the pipeline output will be written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import DataType\n",
    "column_dictionary = {\n",
    "    'Store':DataType.to_string(),\n",
    "    'Brand':DataType.to_string(),\n",
    "    'Quantity':DataType.to_long(),\n",
    "    'Advert':DataType.to_string(),\n",
    "    'Price':DataType.to_float(),\n",
    "    'Revenue':DataType.to_float(),\n",
    "}\n",
    "\n",
    "training_data = OutputFileDatasetConfig(name='Training_Data', destination=(default_ds, 'training_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary)\n",
    "forecasting_data = OutputFileDatasetConfig(name='Forecasting_Data', destination=(default_ds, 'forecasting_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07e9a3",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we define the following parameters for our Azure ML Pipeline:\n",
    "\n",
    "| Parameter Name | Parameter Description |\n",
    "|----------------|-----------------------|\n",
    "| `model_name` | Name of the custom object detection model to be trained (used for model registration). |\n",
    "| `dataset_name` | The name of the dataset to be used for instance segmentation model training within the pipeline. |\n",
    "\n",
    "[PipelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.graph.pipelineparameter?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_relative_path = PipelineParameter(name='datastore_relative_path', default_value='sample_data')\n",
    "raw_dataset_name = PipelineParameter(name='raw_dataset_name', default_value='Raw_Sample_Dataset')\n",
    "train_dataset_name = PipelineParameter(name='train_dataset_name', default_value='Train_Sample_Dataset')\n",
    "forecast_dataset_name = PipelineParameter(name='forecast_dataset_name', default_value='Forecast_Sample_Dataset')\n",
    "result_dataset_name = PipelineParameter(name='result_dataset_name', default_value='Result_Sample_Dataset')\n",
    "timestamp_column = PipelineParameter(name='timestamp_column', default_value='WeekStarting')\n",
    "cutoff_date = PipelineParameter(name='cutoff_date', default_value='1992-05-28')\n",
    "model_name = PipelineParameter(name='model_name', default_value='Sample_Forecasting_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05577c",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of three distinct steps to prepare data, train models, and generate a forecast/register the model to the workspace. First, we call `organize_data.py` and retrieve data from the registered datastore, split into training and forecasting subsets based on the specified `cutoff_date`, save each time-series to a file and register as a new File Dataset. \n",
    "\n",
    "From here we configure an AutoML job forecasting job which will train and return the best performing model for your particular time-series.\n",
    "\n",
    "Following training, we generate a forecast across the dates included in the `forecast_dataset` using the best-performing model.\n",
    "\n",
    "Finally, we aggregate all of the forecasted results across time-series into a single dataset (`result_dataset`) and register that in the AML datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bed6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PythonScriptStep to gather data from remote source and register as AML dataset\n",
    "organize_data_step = PythonScriptStep(\n",
    "    name='Organize Time-Series Data',\n",
    "    script_name=\"get_data.py\", \n",
    "    arguments=[\"--raw_dataset_name\", raw_dataset_name, \n",
    "               \"--train_dataset_name\", train_dataset_name, \n",
    "               \"--forecast_dataset_name\", forecast_dataset_name, \n",
    "               \"--datastore_relative_path\", datastore_relative_path, \n",
    "              '--timestamp_column', timestamp_column,\n",
    "              '--cutoff_date', cutoff_date,\n",
    "              '--training_data', training_data,\n",
    "              '--forecasting_data', forecasting_data],\n",
    "    outputs=[training_data, forecasting_data],\n",
    "    compute_target=compute_target, \n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "automl_config = AutoMLConfig(\n",
    "    task= 'forecasting',\n",
    "    primary_metric= 'normalized_root_mean_squared_error',\n",
    "    iteration_timeout_minutes= 60,\n",
    "    iterations = 30,\n",
    "    experiment_timeout_hours= 3,\n",
    "    label_column_name= 'Quantity',\n",
    "    n_cross_validations= 3,\n",
    "    debug_log='automl_sales_debug.txt',\n",
    "    time_column_name= 'WeekStarting',\n",
    "    max_horizon = 20,\n",
    "    compute_target = compute_target,\n",
    "    training_data=training_data)\n",
    "\n",
    "train_model_step = AutoMLStep(name='Train Forecasting Model (AutoML)',\n",
    "    automl_config=automl_config,\n",
    "    passthru_automl_config=False,\n",
    "    enable_default_model_output=False,\n",
    "    enable_default_metrics_output=False,\n",
    "    allow_reuse=False)\n",
    "\n",
    "# #Evaluate and register\n",
    "register_step = PythonScriptStep(\n",
    "    name = 'Register Model and Generate Forecast',\n",
    "    script_name='register.py',\n",
    "    inputs=[forecasting_data.as_input(name='Forecasting_Data')],\n",
    "    arguments=['--model_name', model_name, '--target_column', 'Quantity', '--result_dataset_name', result_dataset_name],\n",
    "    compute_target=compute_target,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "register_step.run_after(train_model_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65017298",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Pipelines are reusable in AML workflows that can be triggered in multiple ways (manual, programmatic, scheduled, etc.) Create an Azure ML Pipeline by specifying the pipeline steps to be executed.\n",
    "\n",
    "[What are Machine Learning Pipelines?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3869500",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[organize_data_step, train_model_step, register_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee66e3",
   "metadata": {},
   "source": [
    "### Create Published PipelineEndpoint\n",
    "`PipelineEndpoints` can be used to create a versions of published pipelines while maintaining a consistent endpoint. These endpoint URLs can be triggered remotely by submitting an authenticated request and updates to the underlying pipeline are tracked in the AML workspace.\n",
    "\n",
    "[PipelineEndpoint](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline_endpoint.pipelineendpoint?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d49544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=\"AML Pipeline for training forecasting models using AutoML.\",\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # create PipelineEndpoint if it doesn't exist\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_endpoint_name = 'Time-Series Forecast Model Training'\n",
    "pipeline_endpoint_description = 'AML Pipeline for training forecasting models using AutoML'\n",
    "\n",
    "published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                     description=pipeline_endpoint_description,\n",
    "                                     continue_on_step_failure=False)\n",
    "\n",
    "published_pipeline_to_pipeline_endpoint(\n",
    "    workspace=ws,\n",
    "    published_pipeline=published_pipeline,\n",
    "    pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=pipeline_endpoint_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2420e",
   "metadata": {},
   "source": [
    "### Optional: Trigger a Pipeline execution from the notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'sample-automl-forecasting-run')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
